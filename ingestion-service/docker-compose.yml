version: '3.8'

services:
  # Main ingestion service
  ingestion-service:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # Application settings
      APP_NAME: "News Ingestion & Normalization Service"
      APP_VERSION: "1.0.0"
      DEBUG: "False"
      ENVIRONMENT: "production"
      HOST: "0.0.0.0"
      PORT: "8000"
      WORKERS: "2"
      
      # Google Cloud settings
      GCP_PROJECT_ID: ${GCP_PROJECT_ID}
      GCP_REGION: ${GCP_REGION:-us-central1}
      GOOGLE_APPLICATION_CREDENTIALS: "/app/credentials/gcp-key.json"
      
      # Pub/Sub settings
      PUBSUB_TOPIC_INGESTION: "news-ingestion"
      PUBSUB_TOPIC_NORMALIZATION: "news-normalization"
      PUBSUB_SUBSCRIPTION_INGESTION: "news-ingestion-sub"
      
      # Firestore settings
      FIRESTORE_COLLECTION_ARTICLES: "articles"
      FIRESTORE_COLLECTION_SOURCES: "sources"
      FIRESTORE_COLLECTION_METADATA: "metadata"
      
      # Redis settings
      REDIS_URL: "redis://redis:6379/0"
      REDIS_MAX_CONNECTIONS: "20"
      
      # Content processing settings
      MAX_CONTENT_LENGTH: "10485760"
      MAX_ARTICLE_AGE_DAYS: "30"
      MIN_WORD_COUNT: "50"
      MAX_WORD_COUNT: "50000"
      
      # Rate limiting settings
      DEFAULT_RATE_LIMIT: "60"
      RATE_LIMIT_BACKOFF_FACTOR: "2.0"
      MAX_RETRY_ATTEMPTS: "3"
      RETRY_DELAY_SECONDS: "1"
      
      # HTTP client settings
      HTTP_TIMEOUT: "30"
      HTTP_MAX_CONNECTIONS: "100"
      HTTP_KEEPALIVE_TIMEOUT: "5"
      USER_AGENT: "NewsBot/1.0 (+https://example.com/bot)"
      
      # Content normalization settings
      DEFAULT_LANGUAGE: "en"
      SUPPORTED_LANGUAGES: '["en", "es", "fr", "de", "it", "pt"]'
      TIMEZONE: "UTC"
      
      # Duplicate detection settings
      DUPLICATE_THRESHOLD: "0.8"
      CONTENT_HASH_ALGORITHM: "sha256"
      
      # Web scraping settings
      ENABLE_WEB_SCRAPING: "True"
      SCRAPING_TIMEOUT: "30"
      SCRAPING_DELAY: "1.0"
      RESPECT_ROBOTS_TXT: "True"
      
      # Monitoring settings
      ENABLE_METRICS: "True"
      METRICS_PATH: "/metrics"
      LOG_LEVEL: "INFO"
      LOG_FORMAT: "json"
      
      # Health check settings
      HEALTH_CHECK_PATH: "/health"
      HEALTH_CHECK_DEPENDENCIES: "True"
      
      # Content sources settings
      SOURCES_CONFIG_PATH: "config/sources.yaml"
      ENABLE_RSS_FEEDS: "True"
      ENABLE_JSON_FEEDS: "True"
      ENABLE_API_SOURCES: "True"
      ENABLE_WEB_SCRAPING: "True"
      
      # Performance settings
      BATCH_SIZE: "100"
      MAX_CONCURRENT_TASKS: "50"
      PROCESSING_INTERVAL: "60"
      
      # Security settings
      ALLOWED_DOMAINS: "[]"
      BLOCKED_DOMAINS: "[]"
      ALLOWED_CONTENT_TYPES: '["text/html", "text/xml", "application/xml", "application/rss+xml", "application/atom+xml", "application/json"]'
    
    volumes:
      - ./config:/app/config
      - ./credentials:/app/credentials:ro
      - ./logs:/app/logs
      - ./data:/app/data
    
    depends_on:
      - redis
      - prometheus
      - grafana
    
    networks:
      - ingestion-network
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis for caching and rate limiting
  redis:
    image: redis:7.0-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
      - ./deployment/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - ingestion-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:v2.45.0
    ports:
      - "9090:9090"
    volumes:
      - ./deployment/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - ingestion-network
    restart: unless-stopped

  # Grafana for metrics visualization
  grafana:
    image: grafana/grafana:10.0.0
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./deployment/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./deployment/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - ingestion-network
    restart: unless-stopped
    depends_on:
      - prometheus

  # Nginx reverse proxy
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./deployment/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./deployment/ssl:/etc/nginx/ssl:ro
    networks:
      - ingestion-network
    restart: unless-stopped
    depends_on:
      - ingestion-service

  # Log aggregation with Fluentd
  fluentd:
    image: fluent/fluentd:v1.16-debian-1
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    volumes:
      - ./deployment/fluentd.conf:/fluentd/etc/fluent.conf:ro
      - ./logs:/var/log/fluentd
    networks:
      - ingestion-network
    restart: unless-stopped

  # Elasticsearch for log storage
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - ingestion-network
    restart: unless-stopped

  # Kibana for log visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - ingestion-network
    restart: unless-stopped
    depends_on:
      - elasticsearch

volumes:
  redis_data:
  prometheus_data:
  grafana_data:
  elasticsearch_data:

networks:
  ingestion-network:
    driver: bridge
